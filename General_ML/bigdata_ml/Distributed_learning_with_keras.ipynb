{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Distributed learning with keras.ipynb",
      "version": "0.3.2",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1xUEBly5NX5l",
        "colab_type": "text"
      },
      "source": [
        "# Create, Tune, Deploy and Scale a Deep Neural Network in 9 Minutes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GRdh6PYYPM-s",
        "colab_type": "text"
      },
      "source": [
        "<h2>Import Modules</h2>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R91Mfmc7NT9c",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "outputId": "13468a53-93c3-42c1-e556-5c8e9d25fbc2"
      },
      "source": [
        "\n",
        "import keras\n",
        "print('Current:\\t', keras.__version__)\n",
        "print('Expected:\\t 2.1.3')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Current:\t 2.2.4\n",
            "Expected:\t 2.1.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6FE6SvwVNlHi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "be509cd4-fabc-4ade-ef2b-3b2fe926bbb5"
      },
      "source": [
        "import tensorflow as tf\n",
        "print('Current:\\t', tf.__version__)\n",
        "print('Expected:\\t 1.5.0')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Current:\t 1.14.0-rc1\n",
            "Expected:\t 1.5.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pv_WR47rNvfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras import backend as K\n",
        "\n",
        "K.set_session(K.tf.Session(config=K.tf.ConfigProto(intra_op_parallelism_threads=1, inter_op_parallelism_threads=1)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Emjwp_sSN5Pn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import keras\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Dense\n",
        "from keras.layers import Dense, Dropout, Flatten\n",
        "from keras.layers import Conv2D, MaxPooling2D\n",
        "from keras.datasets import mnist\n",
        "from keras.models import Sequential, load_model\n",
        "from keras.optimizers import RMSprop\n",
        "from keras.layers import ReLU\n",
        "\n",
        "from keras import backend as K\n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n4DNDJZCPVg0",
        "colab_type": "text"
      },
      "source": [
        "<h1>Converting data into Vectors</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FMh0_PUAOGKu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "aea20e13-f837-4692-ad54-e6eba666cb51"
      },
      "source": [
        "batch_size = 128\n",
        "num_classes = 10\n",
        "epochs = 1\n",
        "\n",
        "# the data, split between train and test sets\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "x_train = x_train.reshape(60000, 784)\n",
        "x_test = x_test.reshape(10000, 784)\n",
        "x_train = x_train.astype('float32')\n",
        "x_test = x_test.astype('float32')\n",
        "x_train /= 255\n",
        "x_test /= 255\n",
        "\n",
        "\n",
        "# convert class vectors to binary class matrices\n",
        "y_train = keras.utils.to_categorical(y_train, num_classes)\n",
        "y_test = keras.utils.to_categorical(y_test, num_classes)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bvw_2ojqPc43",
        "colab_type": "text"
      },
      "source": [
        "<h1>Single Model </h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfLsJ6mMOPAt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 428
        },
        "outputId": "f659b95b-24c9-4c52-ad42-2f3218f129a7"
      },
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(512, input_shape=(784,)))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "        optimizer='rmsprop',\n",
        "        metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train, y_train,\n",
        "        batch_size=batch_size,\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        validation_data=(x_test, y_test))\n",
        "        \n",
        "score = model.evaluate(x_test, y_test, verbose=0)\n",
        "\n",
        "print('\\n')\n",
        "print('Accuracy:',score[1])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0624 02:03:46.402806 139674275665792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0624 02:03:46.407894 139674275665792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0624 02:03:46.419222 139674275665792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
            "\n",
            "W0624 02:03:46.453610 139674275665792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0624 02:03:46.481587 139674275665792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3295: The name tf.log is deprecated. Please use tf.math.log instead.\n",
            "\n",
            "W0624 02:03:46.614163 139674275665792 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
            "W0624 02:03:46.684950 139674275665792 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:986: The name tf.assign_add is deprecated. Please use tf.compat.v1.assign_add instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 6s 106us/step - loss: 0.3856 - acc: 0.8878 - val_loss: 0.3116 - val_acc: 0.9116\n",
            "\n",
            "\n",
            "Accuracy: 0.9116\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "__WKrumbPkNT",
        "colab_type": "text"
      },
      "source": [
        "<h1>Distributed Learning</h1>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ie9Tw_9FOYlr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "outputId": "38f799b4-3bd4-41a2-967a-9e58fa7f3aa9"
      },
      "source": [
        "activation_functions_layer_1 = ['sigmoid','tanh','relu']\n",
        "opimizers = ['rmsprop','adagrad','adadelta']\n",
        "\n",
        "#optimize over parameter grid (grid search)\n",
        "\n",
        "for activation_function_layer_1 in activation_functions_layer_1:\n",
        "    for opimizer in opimizers:\n",
        "        \n",
        "        model = Sequential()\n",
        "        model.add(Dense(512, activation = activation_function_layer_1, input_shape=(784,)))\n",
        "        model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "\n",
        "\n",
        "        model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=opimizer,\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "        model.fit(x_train, y_train,\n",
        "              batch_size=batch_size,\n",
        "              epochs=epochs,\n",
        "              verbose=1,\n",
        "              validation_data=(x_test, y_test))\n",
        "        \n",
        "        score = model.evaluate(x_test, y_test, verbose=0)\n",
        "        save_path = \"ker_func_mnist_model_2.%s.%s.%s.h5\" % (activation_function_layer_1,opimizer,score[1])\n",
        "        model.save(save_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 7s 114us/step - loss: 0.4227 - acc: 0.8862 - val_loss: 0.2569 - val_acc: 0.9240\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.4300 - acc: 0.8859 - val_loss: 0.2941 - val_acc: 0.9162\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.5406 - acc: 0.8616 - val_loss: 0.3279 - val_acc: 0.9061\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 6s 102us/step - loss: 0.3356 - acc: 0.9019 - val_loss: 0.2447 - val_acc: 0.9283\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.3355 - acc: 0.9058 - val_loss: 0.2261 - val_acc: 0.9360\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 8s 126us/step - loss: 0.3627 - acc: 0.8946 - val_loss: 0.2843 - val_acc: 0.9116\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 6s 104us/step - loss: 0.2587 - acc: 0.9244 - val_loss: 0.1473 - val_acc: 0.9530\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 6s 95us/step - loss: 0.2384 - acc: 0.9312 - val_loss: 0.1356 - val_acc: 0.9610\n",
            "Train on 60000 samples, validate on 10000 samples\n",
            "Epoch 1/1\n",
            "60000/60000 [==============================] - 8s 131us/step - loss: 0.2968 - acc: 0.9158 - val_loss: 0.1706 - val_acc: 0.9502\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIGhEqOIOq6A",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}